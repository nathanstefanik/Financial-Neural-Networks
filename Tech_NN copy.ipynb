{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38864bittfconda439d724d03a14cccb62e5c349d766dfb",
   "display_name": "Python 3.8.8 64-bit ('tf': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "983e971ae9725c71b70c6a7675bb8947736d684064eaeeb404e8bfaa8fd7741e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating a neural network to predict AAPL red or green days\n",
    "In the third run of building neural network models, I explore a breadth-wise approach to training data. We consider how the model performs when trained on data from Apple Inc and other tech giants. Ancillary to this notebook, I will also train a model focusing on a depth-wise approach where we feed the model more data about a security, beyond the quantitative features of its trading stock. From these two, hopefully we get a better picture of how to approach using AI as a tool for trading.\n",
    "\n",
    "## Considerations\n",
    "### Securities\n",
    "* AAPL\n",
    "* MSFT\n",
    "* AMZN\n",
    "* GOOG\n",
    "* FB\n",
    "* TSLA\n",
    "### Features\n",
    "* price at open\n",
    "* highest traded price on day\n",
    "* lowest traded price on day\n",
    "* price at close (raw)\n",
    "* adjusted close (accounting for after-market actions)\n",
    "* trading volume on day\n",
    "\n",
    "### Goal\n",
    "Given a list of the above features of multiple securities, we wish for the neural network to predict a red or green (current) day for AAPL from a list of 10 previous days of data. In this model, green days are exclusively upward price movement.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Get data\n",
    "Downloading historical data from yahoo finance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "source": [
    "We consider market data from 2014-2018"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2014-01-01'\n",
    "end_date = '2019-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_normalize(arr):\n",
    "    scaling_factor = arr[0]**(-1)\n",
    "    answer = [round(i*scaling_factor,8) for i in arr]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = yf.download('AAPL', start=start_date, end=end_date, progress=False)\n",
    "aapl.rename(columns={'Close':'AAPL_Close'}, inplace=True)\n",
    "msft = yf.download('MSFT', start=start_date, end=end_date, progress=False)\n",
    "amzn = yf.download('AMZN', start=start_date, end=end_date, progress=False)\n",
    "goog = yf.download('GOOG', start=start_date, end=end_date, progress=False)\n",
    "fb = yf.download('FB', start=start_date, end=end_date, progress=False)\n",
    "tsla = yf.download('TSLA', start=start_date, end=end_date, progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined = pd.concat([aapl, msft, amzn, goog, fb, tsla], axis=1)"
   ]
  },
  {
   "source": [
    "## 2. Format data\n",
    "We wish to create (10 day by 36 feature) arrays that are normalized based on their column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "num_prev_days = 50\n",
    "num_features = len(combined.axes[1])\n",
    "def create_training_data(df):\n",
    "    prepped = []\n",
    "    for i in range(len(df) - num_prev_days - 1):\n",
    "        # getting data into chunks for normalization\n",
    "        data = [ [] for i in range(num_features) ]\n",
    "        for j in range(num_prev_days):\n",
    "            row = df.values[i + j]\n",
    "            for k in range(num_features):\n",
    "                data[k].append(row[k])\n",
    "        # normalizing using my_normalize\n",
    "        normed = [ my_normalize(data[i]) for i in range(num_features) ]\n",
    "        # rebuilding into a single row\n",
    "        prepping = [ [] for i in range(num_prev_days) ]\n",
    "        for a in range(num_prev_days):\n",
    "            for b in range(num_features):\n",
    "                prepping[a].append(normed[b][a])\n",
    "        # calculating 0 = red or 1 = green\n",
    "        delta = df['AAPL_Close'][i+num_prev_days] - df['AAPL_Close'][i+num_prev_days-1]\n",
    "        result = (0, 1) [delta > 0]\n",
    "        prepped.append([prepping, result])\n",
    "    return prepped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-7-a49116b7122a>:22: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  result = (0, 1) [delta > 0]\n"
     ]
    }
   ],
   "source": [
    "training_data = create_training_data(combined)"
   ]
  },
  {
   "source": [
    "### Randomize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "source": [
    "## 3. Create model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x_train = X\n",
    "y_train = y"
   ]
  },
  {
   "source": [
    "### Layers\n",
    "* Input: flatten\n",
    "* Hidden: 2 layer, 50 neurons, rectified linear unit activation function\n",
    "* Output: softmax\n",
    "\n",
    "Notes: By tinkering, I found this to be a solid configuration while avoiding overfitting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(50, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(50, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))"
   ]
  },
  {
   "source": [
    "### Optimizer and loss function\n",
    "* adam - This is a stochastic gradient descent method, based on an \"adaptive estimation of first-order and second-order moments.\" (I may write a walkthrough on SGD. Given a random variable $X$ and integer $k>0$, $k$-th moments are $\\mathbb{E}(x^k)$.)\n",
    "* binary crossentropy - This loss function is useful in binary classification. I am not using it to its full potential in this model, but binary crossentropy can be very helpful when we wish to train multiple binary classifiers.\n",
    "    * The specific formula for calculating loss is given below.\n",
    "    * $\\mathrm{Loss} = - \\frac{1}{\\mathrm{output \\atop size}} \\sum_{i=1}^{\\mathrm{output \\atop size}} y_i \\cdot \\mathrm{log}\\; {\\hat{y}}_i + (1-y_i) \\cdot \\mathrm{log}\\; (1-{\\hat{y}}_i)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "## 4. Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.8410 - accuracy: 0.5136\n",
      "Epoch 2/3\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.7189 - accuracy: 0.5096\n",
      "Epoch 3/3\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.7034 - accuracy: 0.5259\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14dddaa60>"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "source": [
    "## 5. Evaluating model from prices on more recent days\n",
    "Using closing prices from the most recent year"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import date\n",
    "# from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = '2019-01-01'\n",
    "new_aapl = yf.download('AAPL', start=test_start, end=date.today(), progress=False)\n",
    "new_aapl.rename(columns={'Close':'AAPL_Close'}, inplace=True)\n",
    "new_msft = yf.download('MSFT', start=test_start, end=date.today(), progress=False)\n",
    "new_amzn = yf.download('AMZN', start=test_start, end=date.today(), progress=False)\n",
    "new_goog = yf.download('GOOG', start=test_start, end=date.today(), progress=False)\n",
    "new_fb = yf.download('FB', start=test_start, end=date.today(), progress=False)\n",
    "new_tsla = yf.download('TSLA', start=test_start, end=date.today(), progress=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_combined = pd.concat([new_aapl, new_msft, new_amzn, new_goog, new_fb, new_tsla], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-7-a49116b7122a>:22: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  result = (0, 1) [delta > 0]\n"
     ]
    }
   ],
   "source": [
    "val1 = create_training_data(new_combined)\n",
    "X_eval1 = []\n",
    "y_eval1 = []\n",
    "for features, label in val1:\n",
    "    X_eval1.append(features)\n",
    "    y_eval1.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17/17 [==============================] - 0s 453us/step - loss: 0.6943 - accuracy: 0.5508\n",
      "0.6942554116249084\n",
      "0.5508317947387695\n"
     ]
    }
   ],
   "source": [
    "val1_loss, val1_acc = model.evaluate(X_eval1, y_eval1)\n",
    "print(val1_loss)\n",
    "print(val1_acc)"
   ]
  },
  {
   "source": [
    "### Is it better than guessing?\n",
    "A sanity check. Run this multiple times to develop an idea of how a blind guessing strategy would perform."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 0\n",
    "for day in y_eval1:\n",
    "    if day == random.randint(0,1):\n",
    "        right += 1\n",
    "guessing = right/len(y_eval1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.49353049907578556\n"
     ]
    }
   ],
   "source": [
    "print(guessing)"
   ]
  },
  {
   "source": [
    "## 6. Conclusions\n",
    "Creating this notebook decreased my confidence in using AI by this approach. The addition of a greater breadth of similar numerical data did not seem to help the model. I discovered a major flaw in my previous AAPL_NN notebook in that the normalization process depreciated the value of all features except volume. After fixing that, I will go ahead and work on the fourth notebook that will look at a greater depth of data. Further, I will explore categorical data and look into KNN algorithms in the fourth notebook, as well as working on implementing PCA on this third notebook.\n",
    "\n",
    "I believe that the lack of improvement in this third notebook lies in the lack of value the additional data brings. PCA will help with pruning the data, as well as quicken the computations. Thus, this look into categorical data on AAPL will hopefully bring about new information.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}