{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38864bittensorflowconda3cec8169a2dd40a9aae39c1789a0901b",
   "display_name": "Python 3.8.8 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating a nerual network to predict AAPL red or green days\n",
    "This version considers \n",
    "* price at open\n",
    "* highest traded price on day\n",
    "* lowest traded price on day\n",
    "* price at close (raw)\n",
    "* adjusted close (accounting for after-market actions)\n",
    "* trading volume on day\n",
    "\n",
    "**Goal**: Given a list of the above features of a security, we wish for the neural network to predict a red or green (current) day from a list of 10 previous days. In this model, green days are exclusively upward price movement.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Get data\n",
    "Downloading historical data from yahoo finance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "source": [
    "We consider market data from 2014-2018"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2014-01-01'\n",
    "end_date = '2018-12-31'\n",
    "aapl = yf.download('AAPL', start=start_date, end=end_date, progress=False)"
   ]
  },
  {
   "source": [
    "### Preprocessing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "num_prev_days = 10\n",
    "def create_training_data(df):\n",
    "    prepped = []\n",
    "    for i in range(0,len(df) - num_prev_days - 1):\n",
    "        normed = preprocessing.normalize(df[i:i+num_prev_days])\n",
    "        delta = df['Close'][i+num_prev_days] - df['Close'][i+num_prev_days-1]\n",
    "        result = ([0,1], [1,0]) [delta > 0]\n",
    "        prepped.append([normed.tolist(), result])\n",
    "    return prepped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = create_training_data(aapl)"
   ]
  },
  {
   "source": [
    "### Randomize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "source": [
    "## Creating model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x_train = X\n",
    "y_train = y"
   ]
  },
  {
   "source": [
    "### Layers\n",
    "* Input: flatten\n",
    "* Hidden: 1 layer, 10 neurons, rectified linear unit activation function\n",
    "* Output: softmax\n",
    "\n",
    "Notes: I used the same configuration as my last build on XLY."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Optimizer and loss function\n",
    "* adam - This is a stochastic gradient descent method, based on an \"adaptive estimation of first-order and second-order moments.\" (I may write a walkthrough on SGD. Given a random variable $X$ and integer $k>0$, $k$-th moments are $\\mathbb{E}(x^k)$.)\n",
    "* binary crossentropy - This loss function is useful in binary classification. I am not using it to its full potential in this model, but binary crossentropy can be very helpful when we wish to train multiple binary classifiers.\n",
    "    * The specific formula for calculating loss is given below.\n",
    "    * $\\mathrm{Loss} = - \\frac{1}{\\mathrm{output \\atop size}} \\sum_{i=1}^{\\mathrm{output \\atop size}} y_i \\cdot \\mathrm{log}\\; {\\hat{y}}_i + (1-y_i) \\cdot \\mathrm{log}\\; (1-{\\hat{y}}_i)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "39/39 [==============================] - 0s 455us/step - loss: 0.8499 - accuracy: 0.4875\n",
      "Epoch 2/3\n",
      "39/39 [==============================] - 0s 404us/step - loss: 0.7122 - accuracy: 0.5152\n",
      "Epoch 3/3\n",
      "39/39 [==============================] - 0s 353us/step - loss: 0.6925 - accuracy: 0.5238\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa5ca5630a0>"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "source": [
    "## Evaluating model from prices on more recent days\n",
    "### Evaluation 1\n",
    "Using closing prices from the most recent year"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import date\n",
    "# from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1_aapl = yf.download('AAPL',\n",
    "start='2020-01-01',\n",
    "end=date.today(),\n",
    "progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = create_training_data(new1_aapl)\n",
    "X_eval1 = []\n",
    "y_eval1 = []\n",
    "for features, label in val1:\n",
    "    X_eval1.append(features)\n",
    "    y_eval1.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11/11 [==============================] - 0s 426us/step - loss: 0.6918 - accuracy: 0.5274\n",
      "0.6918272376060486\n",
      "0.5274389982223511\n"
     ]
    }
   ],
   "source": [
    "val1_loss, val1_acc = model.evaluate(X_eval1, y_eval1)\n",
    "print(val1_loss)\n",
    "print(val1_acc)"
   ]
  },
  {
   "source": [
    "### Evaluation 2\n",
    "Using closing prices from 2019"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_aapl = yf.download('AAPL',\n",
    "start='2019-01-01',\n",
    "end='2020-01-01',\n",
    "progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2 = create_training_data(new2_aapl)\n",
    "X_eval2 = []\n",
    "y_eval2 = []\n",
    "for features,label in val2:\n",
    "    X_eval2.append(features)\n",
    "    y_eval2.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8/8 [==============================] - 0s 436us/step - loss: 0.6883 - accuracy: 0.5768\n",
      "0.6883175373077393\n",
      "0.5767635107040405\n"
     ]
    }
   ],
   "source": [
    "val2_loss, val2_acc = model.evaluate(X_eval2, y_eval2)\n",
    "print(val2_loss)\n",
    "print(val2_acc)"
   ]
  },
  {
   "source": [
    "### Conclusions\n",
    "Considering this past year's technology market, it was not unexpected to see worse performance on the evaluation from this past year's data. Running an evaluation on an earlier year (2019) did show improvement. Comparing this with my previous work on XLY, I still believe increasing the dimensionality of the data will improve neural network models. In future builds, I wish to add features that are not directly related to price. With increased dimensionality, I will also be studying PCA to help with the preprocessing.\n",
    "\n",
    "### Future steps\n",
    "I am currently looking at dynamic pricing models to study different perspectives on how we should consider securities. Additionally, I am interested in the portfolio management problem, and how neural networks may help us choose our actions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}